{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-07T18:13:17.007103Z",
     "iopub.status.busy": "2025-05-07T18:13:17.006368Z",
     "iopub.status.idle": "2025-05-07T18:13:20.975830Z",
     "shell.execute_reply": "2025-05-07T18:13:20.974488Z",
     "shell.execute_reply.started": "2025-05-07T18:13:17.007076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.58)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.24.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.19.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.13.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.26.4->langchain) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.26.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:12:29.006693Z",
     "iopub.status.busy": "2025-05-07T18:12:29.006338Z",
     "iopub.status.idle": "2025-05-07T18:12:29.033940Z",
     "shell.execute_reply": "2025-05-07T18:12:29.032350Z",
     "shell.execute_reply.started": "2025-05-07T18:12:29.006664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SimilarityResult(BaseModel):\n",
    "    \"\"\"Output schema for semantic similarity analysis.\"\"\"\n",
    "    entity_match_score: float = Field(..., description=\"Score for how well entities match between texts (0-1)\")\n",
    "    concept_match_score: float = Field(..., description=\"Score for how well main concepts match (0-1)\")\n",
    "    context_match_score: float = Field(..., description=\"Score for how well context and details match (0-1)\")\n",
    "    perspective_match_score: float = Field(..., description=\"Score for how well tone and perspective match (0-1)\")\n",
    "    final_similarity_score: float = Field(..., description=\"Overall semantic similarity score (0-5)\")\n",
    "    reasoning: str = Field(..., description=\"Detailed analysis explaining the similarity assessment\")\n",
    "\n",
    "\n",
    "class SemanticSimilarityAnalyzer:\n",
    "    \"\"\"A class for analyzing semantic similarity between texts using LLM reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"llama3-8b-8192\",  \n",
    "        api_key: Optional[str] = \"api_key\",\n",
    "        temperature: float = 0.1,  # Low temperature for more consistent reasoning\n",
    "        max_retries: int = 3,\n",
    "        retry_delay: int = 2\n",
    "    ):\n",
    "        \"\"\"Initialize the semantic similarity analyzer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the Groq model to use\n",
    "            api_key: Groq API key (if None, tries to get from environment)\n",
    "            temperature: Temperature setting for generation\n",
    "            max_retries: Maximum number of retries for API calls\n",
    "            retry_delay: Delay in seconds between retries\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key must be provided or set as GROQ_API_KEY environment variable\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            model_name=model_name,\n",
    "            api_key=self.api_key,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.output_parser = PydanticOutputParser(pydantic_object=SimilarityResult)\n",
    "        \n",
    "    def _create_prompt(self, text1: str, text2: str) -> str:\n",
    "        \"\"\"Create a detailed prompt for semantic similarity analysis.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert in semantic analysis with deep understanding of language nuance.\n",
    "\n",
    "Task: Analyze the semantic similarity between two texts thoroughly, then provide structured scoring.\n",
    "\n",
    "Text A: \"{text1}\"\n",
    "\n",
    "Text B: \"{text2}\"\n",
    "\n",
    "Approach your analysis methodically:\n",
    "\n",
    "1. Entity Analysis: Identify key entities (people, objects, concepts) in both texts. Are they the same? Similar? Different?\n",
    "\n",
    "2. Concept Mapping: Extract and compare the main ideas, themes, and purposes of both texts.\n",
    "\n",
    "3. Context Evaluation: Analyze details, examples, time references, locations, and supporting information.\n",
    "\n",
    "4. Perspective Assessment: Compare tone, viewpoint, implied stance, and emotional content.\n",
    "\n",
    "Through your analysis, assign scores for each dimension (0.0-1.0) and provide a final similarity score (0.0-5.0).\n",
    "\n",
    "{self.output_parser.get_format_instructions()}\n",
    "\n",
    "Remember that genuinely similar texts should convey essentially the same core meaning, even if they use different words or structures.\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def analyze_similarity(self, text1: str, text2: str) -> SimilarityResult:\n",
    "        \"\"\"Analyze the semantic similarity between two texts.\n",
    "        \n",
    "        Args:\n",
    "            text1: First text for comparison\n",
    "            text2: Second text for comparison\n",
    "            \n",
    "        Returns:\n",
    "            SimilarityResult object containing scores and reasoning\n",
    "        \"\"\"\n",
    "        prompt = self._create_prompt(text1, text2)\n",
    "        \n",
    "        # Implement retry logic for API stability\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "                # Extract the JSON part from the response\n",
    "                match = re.search(r'\\{[\\s\\S]*\\}', response.content)\n",
    "                if match:\n",
    "                    json_str = match.group(0)\n",
    "                    return self.output_parser.parse(json_str)\n",
    "                else:\n",
    "                    # If no JSON found, try to handle the raw response\n",
    "                    return self._parse_unstructured_response(response.content)\n",
    "            except Exception as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    print(f\"Attempt {attempt+1} failed with error: {str(e)}. Retrying in {self.retry_delay}s...\")\n",
    "                    time.sleep(self.retry_delay)\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to analyze similarity after {self.max_retries} attempts: {str(e)}\")\n",
    "    \n",
    "    def _parse_unstructured_response(self, content: str) -> SimilarityResult:\n",
    "        \"\"\"Attempt to parse an unstructured response into our schema.\"\"\"\n",
    "        # Extract the final score\n",
    "        score_match = re.search(r'(?:final|overall)?\\s*(?:similarity)?\\s*score:?\\s*(\\d+\\.?\\d*)', \n",
    "                              content.lower(), re.IGNORECASE)\n",
    "        final_score = float(score_match.group(1)) if score_match else 2.5  # Default mid-point\n",
    "        \n",
    "        # Default scores when structured parsing fails\n",
    "        return SimilarityResult(\n",
    "            entity_match_score=final_score/5.0,\n",
    "            concept_match_score=final_score/5.0,\n",
    "            context_match_score=final_score/5.0,\n",
    "            perspective_match_score=final_score/5.0,\n",
    "            final_similarity_score=final_score,\n",
    "            reasoning=content\n",
    "        )\n",
    "    \n",
    "    def batch_analyze(self, text_pairs: List[Tuple[str, str]], batch_size: int = 5) -> List[SimilarityResult]:\n",
    "        \"\"\"Process multiple text pairs in batches.\n",
    "        \n",
    "        Args:\n",
    "            text_pairs: List of (text1, text2) tuples to compare\n",
    "            batch_size: Number of pairs to process before reporting progress\n",
    "            \n",
    "        Returns:\n",
    "            List of SimilarityResult objects\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(text_pairs), batch_size), desc=\"Processing batches\"):\n",
    "            batch = text_pairs[i:i+batch_size]\n",
    "            batch_results = []\n",
    "            \n",
    "            for text1, text2 in batch:\n",
    "                try:\n",
    "                    result = self.analyze_similarity(text1, text2)\n",
    "                    batch_results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing pair: {str(e)}\")\n",
    "                    # Add a placeholder result\n",
    "                    batch_results.append(SimilarityResult(\n",
    "                        entity_match_score=0.0,\n",
    "                        concept_match_score=0.0,\n",
    "                        context_match_score=0.0,\n",
    "                        perspective_match_score=0.0,\n",
    "                        final_similarity_score=0.0,\n",
    "                        reasoning=f\"Error: {str(e)}\"\n",
    "                    ))\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T18:13:40.875813Z",
     "iopub.status.busy": "2025-05-07T18:13:40.875453Z",
     "iopub.status.idle": "2025-05-07T18:13:42.197937Z",
     "shell.execute_reply": "2025-05-07T18:13:42.196866Z",
     "shell.execute_reply.started": "2025-05-07T18:13:40.875785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Similarity Score: 4.5/5.0\n",
      "Entity Match: 0.80/1.0\n",
      "Concept Match: 0.90/1.0\n",
      "Context Match: 0.80/1.0\n",
      "Perspective Match: 1.00/1.0\n",
      "Reasoning: The two texts share a high degree of semantic similarity, with most entities, concepts, and context being identical or very similar. The main ideas and sub-ideas are also very similar, with some minor differences in wording. The tone, viewpoint, and emotional content are identical. The only minor differences are in the specific examples and wording used. Overall, the texts convey essentially the same core meaning, making them highly similar.\n"
     ]
    }
   ],
   "source": [
    "para1 = \"\"\"The implementation of artificial intelligence in healthcare systems has shown promising results \n",
    "in diagnostics and treatment planning. Recent studies indicate that AI algorithms can detect \n",
    "certain conditions with accuracy comparable to that of experienced physicians, while potentially \n",
    "reducing diagnostic time. However, concerns about data privacy and the black-box nature of some \n",
    "AI models remain significant obstacles to widespread adoption.\"\"\"\n",
    "\n",
    "para2 = \"\"\"Healthcare systems have benefited from artificial intelligence applications, particularly \n",
    "in the areas of diagnosis and treatment recommendations. Research demonstrates that machine learning \n",
    "models can identify some medical conditions as accurately as seasoned doctors, often in less time. \n",
    "Nevertheless, issues related to patient privacy and the interpretability of complex AI systems continue \n",
    "to present challenges for broader implementation.\"\"\"\n",
    "\n",
    "\n",
    "analyzer = SemanticSimilarityAnalyzer()\n",
    "\n",
    "\n",
    "result = analyzer.analyze_similarity(para1, para2)\n",
    "\n",
    "\n",
    "print(f\"Final Similarity Score: {result.final_similarity_score}/5.0\")\n",
    "print(f\"Entity Match: {result.entity_match_score:.2f}/1.0\")\n",
    "print(f\"Concept Match: {result.concept_match_score:.2f}/1.0\")\n",
    "print(f\"Context Match: {result.context_match_score:.2f}/1.0\")\n",
    "print(f\"Perspective Match: {result.perspective_match_score:.2f}/1.0\")\n",
    "print(f\"Reasoning: {result.reasoning}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
